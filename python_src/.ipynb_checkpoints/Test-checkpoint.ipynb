{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import csv\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import math\n",
    "import os,sys\n",
    "import random\n",
    "from scipy.signal import butter, lfilter,freqz\n",
    "from scipy.interpolate import interp1d\n",
    "# import numpy as np #numpy\n",
    "import pdb\n",
    "import time #time for sleeps\n",
    "#Functions for importing and adjusting data\n",
    "# from datafunction import addrandomnoise,delay_series,butter_lowpass_filter, reshape_with_timestep,min_max_scaler,absolute_percentage_error,find_soak_time\n",
    "# from retrieveData import get_data\n",
    "#Plotting functions \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.preprocessing import MinMaxScaler #Scaled inputs\n",
    "from sklearn.metrics import mean_squared_error, r2_score#Find errors\n",
    "#Use these to build a LSTM model \n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Dropout\n",
    "from keras import metrics\n",
    "\n",
    "# def convertfile(infile):\n",
    "#     mat = loadmat(infile)\n",
    "#     if(infile.find('outer') > 0):\n",
    "#     #    print(infile.find('outer'))\n",
    "#         mat = np.array(mat['outT'])\n",
    "#     else:\n",
    "#         mat = np.array(mat['centerT'])\n",
    "#     return mat\n",
    "\n",
    "def addrandomnoise(array):\n",
    "    return array + np.random.randn(len(array))\n",
    "\n",
    "def butter_lowpass(cutOff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normalCutoff = cutOff / nyq\n",
    "    b, a = butter(order, normalCutoff, btype='low', analog = False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutOff, fs, order=4):\n",
    "    b, a = butter_lowpass(cutOff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def delay_series(data, data_to_predict, delay_time):\n",
    "    #pass data as data np array\n",
    "    len_row = data.shape[0]\n",
    "    len_col = data.shape[1]\n",
    "    for td in range(0,delay_time):\n",
    "        #for i in range(0, len_col):\n",
    "        #extend all Cols\n",
    "        data = np.append(data, data[-1,:][None], axis = 0 )\n",
    "        data = np.delete(data, 0, axis = 0)\n",
    "    data = np.column_stack((data, data_to_predict))\n",
    "    return data\n",
    "\n",
    "def reduce_data_size(data, new_size):\n",
    "    #will take a 2d array and reduce the size of the cols\n",
    "    #pass data as data np array\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    new_data = np.zeros((new_size,num_col), float)\n",
    "    for i in range(0,num_col):\n",
    "        row = 0\n",
    "        for j in range(0, num_row, int(num_row/new_size)):\n",
    "            new_data[row,i] = data[j,i]\n",
    "            row = row + 1\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def reduce_data_size3d(data, new_size):\n",
    "    #will take a 3d array and reduce the size of the cols\n",
    "    #pass data as data np array\n",
    "    flag_2d = 0\n",
    "    num_row = data.shape[1]\n",
    "    num_col = data.shape[2]\n",
    "    new_data = np.zeros((new_size,num_col), float)\n",
    "    out_data = np.zeros((1,new_size,num_col),float)\n",
    "    for k in range(0, data.shape[0]):\n",
    "        for i in range(0,num_col):\n",
    "            row = 0\n",
    "            for j in range(0, num_row, int(num_row/new_size)):\n",
    "                new_data[row,i] = data[j,i]\n",
    "                row = row + 1\n",
    "        if flag_2d == 0:\n",
    "            flag_2d = 1\n",
    "            out_data[0,:,:] = new_data\n",
    "        else:\n",
    "            out_data = np.vstack((out_data, new_data[None]))\n",
    "    return out_data\n",
    "\n",
    "def reshape_with_timestep(data, samples, time_steps):\n",
    "    num_col = data.shape[2]\n",
    "    num_trials = data.shape[0]\n",
    "    print(data.shape)\n",
    "    new_data = np.zeros((num_trials,samples,time_steps,num_col), float)\n",
    "    for t in range(0,num_trials):\n",
    "        for i in range(0,num_col):\n",
    "            for j in range(0, samples):\n",
    "                for k in range(0, time_steps):\n",
    "                    new_data[t,j,k,i] = data[t,k+(time_steps*j),i]\n",
    "    return new_data\n",
    "\n",
    "def randomize_data(data, num_trials, trial_size):\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    print(num_row)\n",
    "    print(num_col)\n",
    "    new_data = np.zeros((num_row,num_col), float)\n",
    "    data_order = list(range(0,num_trials))\n",
    "    print(data_order)\n",
    "    random.shuffle(data_order)\n",
    "    print(data_order)\n",
    "    for i in range(0,num_col):\n",
    "        for j in range(0,num_trials):\n",
    "            print(new_data[(trial_size*j):(trial_size*(j+1)),i])\n",
    "            print(data_order[j])\n",
    "            print(data[(trial_size*data_order[j]):(trial_size*(data_order[j]+1)),i])\n",
    "            new_data[(trial_size*j):(trial_size*(j+1)),i] = data[(trial_size*data_order[j]):(trial_size*(data_order[j]+1)),i]\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def interpolate_data(data,real_size,new_size):\n",
    "    num_row = data.shape[1]\n",
    "    num_col = data.shape[2]\n",
    "    new_data = np.ones((data.shape[0],new_size,num_col))\n",
    "    new_data = -1 * new_data\n",
    "    index = new_size\n",
    "    for t in range(0, data.shape[0]):\n",
    "        for i in range(0,num_col):\n",
    "            if(real_size[t]-1 >= new_size):\n",
    "                new_data[t,:,i] = data[t,:,i]\n",
    "            else:\n",
    "                x = np.linspace(0,real_size[t],real_size[t])\n",
    "                f = interp1d(x, data[t,:real_size[t],i])\n",
    "                x_new = np.linspace(0,real_size[t],new_size)\n",
    "                new_data[t,:,i] = f(x_new)\n",
    "    return new_data\n",
    "\n",
    "def min_max_scaler(data, input_min, input_max, out_min, out_max):\n",
    "    data_std = (data - input_min) / (input_max - input_min)\n",
    "    data_scaled = data_std * (out_max - out_min) + out_min\n",
    "    return data_scaled\n",
    "\n",
    "def short_term_average(data, short_term_len, tolerance):\n",
    "    length = data.shape[0]\n",
    "    sum = 0\n",
    "    for i in range(1,short_term_len):\n",
    "        #Average\n",
    "        if(i+1 > length):\n",
    "            return -1, 0\n",
    "        sum += abs(data[-i]-data[-1]) / 50\n",
    "    #print(sum)\n",
    "    if(sum < tolerance):\n",
    "        return length, sum\n",
    "    else:\n",
    "        return -1, sum\n",
    "\n",
    "def find_soak_time(target, time, air_T, part_T, tolerance):\n",
    "    lower_margin = target - target*tolerance\n",
    "    upper_margin = target + target*tolerance\n",
    "    print(lower_margin)\n",
    "    print(upper_margin)\n",
    "    for i in range(len(time)):\n",
    "        if (air_T[i] > lower_margin) and (air_T[i] <= upper_margin) and (part_T[i] >= lower_margin):\n",
    "            return time[i]\n",
    "    return time[-1]\n",
    "\n",
    "def absolute_percentage_error(y_true, y_pred): \n",
    "    return (np.abs((y_true - y_pred) / y_true) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#Retrives CSV data from a folder specified by path\n",
    "#Input cols_to_keep is list defining which columns should be loaded in\n",
    "###\n",
    "def get_data(feature_cols,truth_col,path,length, resize):\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    real_size = [] \n",
    "    if (path == None):\n",
    "        print(\"ERROR: Use Command Line arg -p to set path to data\")\n",
    "        raise NotADirectoryError(path)\n",
    "    vstack_flag = 0\n",
    "    for filename in os.listdir(path):\n",
    "        with open(path + filename) as csv_file:\n",
    "            print(filename)\n",
    "            my_data = genfromtxt(csv_file, delimiter=',')\n",
    "            real_size += [my_data.shape[0]]\n",
    "            mydata2 = np.zeros([1,length,len(feature_cols)+1])\n",
    "            iter = 1\n",
    "            pdb.set_trace()\n",
    "            for col in feature_cols:\n",
    "                pdb.set_trace()\n",
    "                mydata2[0,:my_data.shape[0],iter] = my_data[:length,col]\n",
    "                iter = iter + 1\n",
    "            mydata2[0,:my_data.shape[0],0] = my_data[:length,truth_col]\n",
    "            if vstack_flag == 0:\n",
    "                raw_data = mydata2\n",
    "                vstack_flag = 1\n",
    "            else:\n",
    "                raw_data = np.vstack((raw_data,mydata2))\n",
    "    if(resize == True):\n",
    "        raw_data = interpolate_data(raw_data,real_size, length)\n",
    "        print(raw_data.shape)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[2]\n",
      "kalman_max40.csv\n",
      "> <ipython-input-11-998238b5a78f>(20)get_data()\n",
      "-> for col in feature_cols:\n",
      "(Pdb) n\n",
      "> <ipython-input-11-998238b5a78f>(21)get_data()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) col\n",
      "1\n",
      "(Pdb) n\n",
      "> <ipython-input-11-998238b5a78f>(22)get_data()\n",
      "-> mydata2[0,:my_data.shape[0],iter] = my_data[:length,col]\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5a6959765515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpart_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mval_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-998238b5a78f>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(feature_cols, truth_col, path, length, resize)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mmydata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmydata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-998238b5a78f>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(feature_cols, truth_col, path, length, resize)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mmydata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmydata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "###\n",
    "#Takes an input directory and tries to append current dir to it\n",
    "#Checks if the input directory is a directory\n",
    "###\n",
    "def dir_path(string):\n",
    "    string2 = os.getcwd() + string\n",
    "    if os.path.isdir(string2):\n",
    "        return string2\n",
    "    elif os.path.isdir(string):\n",
    "        return string\n",
    "    else:\n",
    "        raise NotADirectoryError(string)\n",
    "###\n",
    "#Implements command line arguemnts\n",
    "#-e is number of epochs\n",
    "#-tp is file path point to test data\n",
    "#-vp is file path point to val data\n",
    "#-o is a output file which model will be saved too\n",
    "#-i is a input file which model is loaded from \n",
    "#-vcol is the col with truth/val data\n",
    "#-stcol is for temp and time data\n",
    "#-srcol is for data which changes on a per run basis \n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('-e','--epochs',type=int, help='Number of epochs', required=True)\n",
    "    #parser.add_argument('-tp', '--test_path', help='file path', type=dir_path,required=True)\n",
    "    #parser.add_argument('-vp', '--val_path', help='file path', type=dir_path,required=True)\n",
    "    parser.add_argument('-i', '--in_file', help='Input file for Network weights',required=False)\n",
    "    #parser.add_argument('-o', '--out_file', help='Output file for Network weights',required=False)\n",
    "    #parser.add_argument('-state','--stateful',type=int, help='stateful flag', required=True)\n",
    "    #parser.add_argument('-w','--window_size',type=int, help='Size of window', required=True)\n",
    "    #parser.add_argument('-vcol','--part_col',type=int, help='Output Cols to verify with', required=True)\n",
    "    #parser.add_argument('-tmcol','--air_temp_col', nargs='+',type=int, help='the air temp col', required=True)\n",
    "    #parser.add_argument('-stcol','--scaled_trial_cols', nargs='+',type=int, help='cols that change during trial', required=False)\n",
    "    parser.add_argument('-srcol','--scaled_run_cols', nargs='+',type=int, help='cols that change per trial', required=False)\n",
    "    #parser.add_argument('-min_temp','--min_temp', type=int, help='min_temp to scale temps to', required=True)\n",
    "    #parser.add_argument('-max_temp','--max_temp', type=int, help='max_temp to scale temps to', required=True)\n",
    "    return parser.parse_args()\n",
    "\n",
    "class args:\n",
    "#     def __init__(epochs, test_path, val_path, in_file, out_file, stateful, window_size, part_col, air_temp_col, scaled_trial_cols, , min_temp, max_temp):\n",
    "    def __init__(self,test_path, val_path, out_file, window_size, part_col, scaled_trial_cols, air_temp_col, min_temp, max_temp, epochs, stateful):\n",
    "        self.epochs = [int(epochs)]\n",
    "        self.test_path = test_path\n",
    "        self.val_path = val_path\n",
    "#         self.in_file = in_file\n",
    "        self.out_file = out_file\n",
    "        self.stateful = [int(stateful)]\n",
    "        self.window_size = [int(window_size)]\n",
    "        self.part_col = [int(part_col)]\n",
    "        self.air_temp_col = [int(air_temp_col)]\n",
    "        self.scaled_trial_cols = [int(scaled_trial_cols)]\n",
    "#         self.scaled_run_cols = int(scaled_run_cols)\n",
    "        self.min_temp = [int(min_temp)]\n",
    "        self.max_temp = [int(max_temp)]\n",
    "    in_file = None\n",
    "    scaled_run_cols = None\n",
    "\n",
    "# parsed_args = parse_arguments()\n",
    "parsed_args = args('../real_data_kalman/train/','../real_data_kalman/test/','weights_train_lstm_simple.h5',1,2,0,1,20,70,1000,0)\n",
    "#init data\n",
    "data_len = 3600\n",
    "only_predict_flag = 0 #Flag to determine if train or ONLY predict\n",
    "local_batch_size = 180 #data_len/20, must be multiple of data_len\n",
    "epochs_end = parsed_args.epochs #Number of epochs to train on\n",
    "scalers = {}\n",
    "val_scalers = {}\n",
    "col_scalers = {}\n",
    "plot_data = 1\n",
    "windows = parsed_args.window_size\n",
    "Neurons = 25\n",
    "temp_min = parsed_args.min_temp\n",
    "temp_max = parsed_args.max_temp\n",
    "statful_flag = parsed_args.stateful\n",
    "in_file_name = parsed_args.in_file\n",
    "out_file_name = parsed_args.out_file\n",
    "if(parsed_args.scaled_trial_cols == None):\n",
    "    train_cols = parsed_args.air_temp_col\n",
    "    len_scaled_trial_cols_arg=None\n",
    "else:\n",
    "    train_cols = parsed_args.air_temp_col + parsed_args.scaled_trial_cols\n",
    "    scaled_trial_cols_arg = parsed_args.scaled_trial_cols\n",
    "    len_scaled_trial_cols_arg = len(parsed_args.scaled_trial_cols)\n",
    "if(parsed_args.scaled_run_cols == None):\n",
    "    scaled_run_cols_arg = []\n",
    "    len_scaled_run_cols_arg = None\n",
    "else:\n",
    "    scaled_run_cols_arg = parsed_args.scaled_run_cols\n",
    "    len_scaled_run_cols_arg = -1*len(scaled_run_cols_arg)\n",
    "    train_cols +=  scaled_run_cols_arg\n",
    "\n",
    "print(train_cols)\n",
    "print(parsed_args.part_col)\n",
    "\n",
    "if((in_file_name) != None):\n",
    "    if os.path.isfile(in_file_name):\n",
    "        only_predict_flag = 1\n",
    "    else:\n",
    "        raise FileNotFoundError(in_file_name)\n",
    "\n",
    "val_data = get_data(train_cols,parsed_args.part_col, parsed_args.val_path, data_len, True)\n",
    "\n",
    "val_scaled = val_data\n",
    "#raw_val_reshape = reshape_with_timestep(val_scaled, 360,10) #360 * 10 is data length 3600\n",
    "if(len_scaled_trial_cols_arg != None):\n",
    "    for i in range(2, val_scaled.shape[2] - len(scaled_run_cols_arg)):\n",
    "        for j in range(val_scaled.shape[0]):    \n",
    "            scalers[j+val_scaled.shape[0]] = MinMaxScaler(feature_range=(0, 1))\n",
    "            val_scaled[j,:, i:i+1] = scalers[j+val_scaled.shape[0]].fit_transform(val_data[j,:,i:i+1])\n",
    "\n",
    "val_scaled[:,:,1] = min_max_scaler(val_data[:,:,1], temp_min, temp_max, 0, 1)\n",
    "val_scaled[:,:,0] = min_max_scaler(val_data[:,:,0], temp_min, temp_max, 0, 1)\n",
    "\n",
    "if(only_predict_flag == 0): \n",
    "    raw_data = get_data(train_cols,parsed_args.part_col, parsed_args.test_path, data_len, True)\n",
    "    scaled = raw_data\n",
    "    scaled[:,:,1] = min_max_scaler(raw_data[:,:,1], temp_min, temp_max, 0, 1)\n",
    "    scaled[:,:,0] = min_max_scaler(raw_data[:,:,0], temp_min, temp_max, 0, 1)\n",
    "    if(len_scaled_trial_cols_arg != None):\n",
    "        for i in range(2, scaled.shape[2] - len(scaled_run_cols_arg)):\n",
    "            for j in range(scaled.shape[0]):\n",
    "                scalers[j+scaled.shape[0]] = MinMaxScaler(feature_range=(0, 1))\n",
    "                scaled[j,:, i:i+1] = scalers[j+scaled.shape[0]].fit_transform(raw_data[j,:,i:i+1])\n",
    "\n",
    "val_scaled_reshape = np.zeros((val_scaled.shape[0], data_len, windows, val_scaled.shape[2]))\n",
    "if(len_scaled_run_cols_arg != None):\n",
    "    for i in range(val_scaled.shape[2]- len(scaled_run_cols_arg), val_scaled.shape[2]):\n",
    "        if(only_predict_flag == 0): \n",
    "            col_scalers[i] = MinMaxScaler(feature_range=(0, 1))\n",
    "            col_scalers[i].fit(np.vstack((raw_data[:,:,i], val_data[:,:,i])))\n",
    "            scaled[:,:,i] = col_scalers[i].transform(raw_data[:,:,i])\n",
    "            val_scaled[:,:,i] = col_scalers[i].transform(val_data[:,:,i])\n",
    "            joblib.dump(col_scalers[i], out_file_name[:-3] + str(i) + \".pkl\") \n",
    "        else:\n",
    "            col_scalers[i] = joblib.load(in_file_name[:-3] + str(i) + \".pkl\") \n",
    "            val_scaled[:,:,i] = col_scalers[i].transform(val_data[:,:,i])\n",
    "\n",
    "for t in range(0,val_scaled.shape[0]):\n",
    "    val_scaled[t,:,:] = delay_series(val_scaled[t,:,1:],val_scaled[t,:,0],0)\n",
    "    \n",
    "for t in range(0,val_scaled.shape[0]):\n",
    "    pdb.set_trace()\n",
    "    for i in range(0,windows):\n",
    "        data = val_scaled[t,:,:]\n",
    "        for td in range(0,i):\n",
    "            #for i in range(0, len_col):\n",
    "            #extend all Cols\n",
    "            data = np.append(data, data[-1,:][None], axis = 0 )\n",
    "            data = np.delete(data, 0, axis = 0)\n",
    "        val_scaled_reshape[t,:,-i,:] = data\n",
    "    print(data[50,0])\n",
    "\n",
    "if(only_predict_flag == 0):\n",
    "    for t in range(0,scaled.shape[0]):\n",
    "        scaled[t,:,:] =  delay_series(scaled[t,:,1:],scaled[t,:,0],0)\n",
    "\n",
    "    scaled_reshape = np.zeros((scaled.shape[0], data_len, windows, scaled.shape[2]))\n",
    "    for t in range(0,scaled.shape[0]):\n",
    "        for i in range(0,windows):\n",
    "            data = scaled[t,:,:]\n",
    "            for td in range(0,i):\n",
    "                #for i in range(0, len_col):\n",
    "                #extend all Cols\n",
    "                data = np.append(data, data[-1,:][None], axis = 0 )\n",
    "                data = np.delete(data, 0, axis = 0)\n",
    "            scaled_reshape[t,:,-i,:] = data\n",
    "        print(data[50,0])\n",
    "    #     for k in range(0, scaled.shape[2]):\n",
    "    #         pyplot.plot(scaled[t,:,k],  label=str(k)) #Inner Temp\n",
    "    #         pyplot.title(\"Train_data\")\n",
    "    # pyplot.legend()\n",
    "    # pyplot.show()  \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(Neurons, batch_input_shape=(local_batch_size,val_scaled_reshape.shape[2], val_scaled_reshape.shape[3]-1),activation='softsign', stateful=statful_flag, return_sequences=False))\n",
    "model.add(Dropout(.0001))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()\n",
    "\n",
    "ad = optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='MSE', optimizer=ad)\n",
    "if(only_predict_flag == 0): \n",
    "    train_trials = scaled_reshape.shape[0]\n",
    "    history_log = {'loss' : [0]*epochs_end*train_trials, \\\n",
    "               'val': [0]*epochs_end*train_trials}\n",
    "    for epo in range(0, epochs_end):\n",
    "        for i in range(0, train_trials):\n",
    "            num = epo*train_trials + i\n",
    "            train = scaled_reshape[i,:,:,:]  #select first trial\n",
    "            test = val_scaled_reshape[epo % val_scaled_reshape.shape[0],:,:,:] \n",
    "            # pyplot.plot(train[:,0,:])\n",
    "            # pyplot.show()\n",
    "            # split into input and outputs\n",
    "            train_X, train_y = train[:,:, :-1], train[:,0, -1]\n",
    "            test_X, test_y = test[:,:, :-1], test[:,0, -1]\n",
    "            #Fit the model for a single epoch on each trial but do this many times\n",
    "            history = model.fit(train_X, train_y, epochs=1, batch_size=local_batch_size, \\\n",
    "                validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "            #Store loss and val loss in these dictionaries \n",
    "            history_log['loss'][num] = history.history['loss'][0]+history_log['loss'][num]  \n",
    "            history_log['val'][num] = history.history['val_loss'][0]+history_log['val'][num]\n",
    "        model.reset_states()\n",
    "    pyplot.title(\"Training Loss Curve\")\n",
    "    pyplot.plot(history_log['loss'], label='train')\n",
    "    pyplot.plot(history_log['val'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    if((out_file_name) == None):\n",
    "       #Just set some default name in case you forget to set filename\n",
    "        out_file_name = \"default_name_weights.h5\"\n",
    "    model.save_weights(out_file_name) #save weights\n",
    "    model.save(\"model_\" + out_file_name) #save entire model\n",
    "    # make a prediction\n",
    "else:\n",
    "    print(in_file_name)\n",
    "    #model.load_weights(in_file_name)\n",
    "    if(in_file_name[:5] == \"model\"):\n",
    "        model = load_model(in_file_name)\n",
    "    else:    \n",
    "        model = load_model(\"model_\" + in_file_name)\n",
    "\n",
    "val_data = get_data(train_cols,parsed_args.part_col, parsed_args.val_path, data_len, True)\n",
    "for i in range(0,val_scaled_reshape.shape[0]):\n",
    "    test = val_scaled_reshape[i % val_scaled_reshape.shape[0],:,:,:] \n",
    "    pyplot.plot( test[:,0,0], label='Part Temperature Truth', linestyle = 'dotted') #Inner Temp\n",
    "    pyplot.plot( test[:,0,1], label='Air Temperature') #Inner Temp\n",
    "    pyplot.plot( test[:,0,2],  label='Part Temperature Prediction')\n",
    "    pyplot.xlabel('Time [s]')\n",
    "    pyplot.ylabel('Temperature [C]')\n",
    "    pyplot.title('Neural Network Prediction')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    test_X, test_y = test[:,:, :-1], test[:,0, -1]\n",
    "    yhat = model.predict(test_X, batch_size = local_batch_size)\n",
    "    test_X = val_scaled_reshape[i,:, 0, :-1]\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1)).fit(inv_yhat)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    # calculate RMSE\n",
    "    r2error = r2_score(inv_y, inv_yhat)\n",
    "    print('Test R2: %.9f' % (1-r2error))\n",
    "    if(plot_data == 1):\n",
    "        inv_yhat_out = min_max_scaler(yhat, 0, 1, temp_min, temp_max)\n",
    "        pyplot.plot( val_data[i,:,2], val_data[i,:,0], label='Part Temperature Truth', linestyle = 'dotted') #Inner Temp\n",
    "        pyplot.plot( val_data[i,:,2],val_data[i,:,1], label='Air Temperature') #Inner Temp\n",
    "        pyplot.plot( val_data[i,:,2],inv_yhat_out,  label='Part Temperature Prediction')\n",
    "        pyplot.xlabel('Time [s]')\n",
    "        pyplot.ylabel('Temperature [C]')\n",
    "        pyplot.title('Neural Network Prediction')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "        percent_error = absolute_percentage_error(inv_yhat_out[:,0],val_data[i,:,0])\n",
    "        pyplot.plot(val_data[i,:,2],percent_error,  label='Part Temperature Percent Error')\n",
    "        plt.ylim(-.5, 16.5)\n",
    "        pyplot.xlabel('Time [s]')\n",
    "        pyplot.ylabel('Percent Error')\n",
    "        pyplot.title(\"Precentage Error LSTM\")\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb.pm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
